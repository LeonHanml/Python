{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "data_train = pd.read_table('D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\weibo_train_data(new)\\\\weibo_train_data.txt',\n",
    "                           sep=\"\\t\",\n",
    "                           header=None, encoding='utf-8')\n",
    "header = ['uid', 'mid', 'time', 'forward_count', 'comment_count', 'like_count', 'content']\n",
    "data_train.columns = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentall =pd.DataFrame( data_train.loc[data_train.forward_count>100])\n",
    "data_train=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentall['url']=0\n",
    "contentall['#']=0\n",
    "contentall['brack']=0\n",
    "contentall['at'] =0\n",
    "contentall['book'] =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "data_train['content'].to_csv('D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\weibo_train_data(new)\\\\weibo_train_data_content.txt')\n",
    "data_train[data_train.forward_count>10]['content'].to_csv('D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\weibo_train_data(new)\\\\weibo_train_data_content_above10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(contentall):\n",
    "    str = contentall['content']\n",
    "    if \"http\" in  str: \n",
    "        contentall['url'] =1 \n",
    " \n",
    "    if \"#\" in  str: contentall['#'] =1 \n",
    "    if \"【\" or \"】\" in str :\n",
    "        contentall[\"brack\"] =1\n",
    "\n",
    "    if \"@\" in str: contentall['at'] =1\n",
    "    strs = str.split(\" \")\n",
    "    strs = [str for str in strs if \"http\" not in str]\n",
    "    strs = [str.replace(\"#\",\"\").replace(\"@\",\"\").replace(\"【\",\"\").replace(\"】\",\"\") for str in strs ]\n",
    "    # tag = \",.，。？[]\"\n",
    "    english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '!', '@',\n",
    "                            '#', '%', '$', '*'] # 自定义英文表单符号列表\n",
    "    english_punctuations2 = [',','.',':',';','?','!','(',')','[',']','@','&','#','%','$','{','}','--','-']\n",
    "    chinese_punctuations = ['，', '。.', '：', '；', '？', '（', '）', '【', '】', '！', '@',\n",
    "                            '#', '%', '￥', '*']\n",
    "    tag = chinese_punctuations + english_punctuations\n",
    "    # [s for s in str if s not in tag]\n",
    "    contentall['content'] = \" \".join(strs)\n",
    "\n",
    "    return contentall\n",
    "\n",
    "# def contains_http(txt):\n",
    "#     if \"http\" in txt:\n",
    "#         return 1\n",
    "#     else:return 0\n",
    "# contentall[\"content\"].apply(contains_http)\n",
    "\n",
    "content_clean= contentall.apply(clean_text,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def find_all_emoji(text):\n",
    "    # listall=[[],[]]\n",
    "    pattern = re.compile(\"\\[.{1,3}\\]\")\n",
    "    pattern_book = re.compile(\"《.*》\")\n",
    "    list_emoji = pattern.findall(text)\n",
    "    list_book = pattern_book.findall(text)\n",
    "    return list_emoji,list_book\n",
    "result =content_clean['content'].apply(find_all_emoji)\n",
    "array_value = result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[+[]]',\n '[1/3]',\n '[3/3]',\n '[5]',\n '[IPN]',\n '[]]',\n '[~+[]',\n '[专利]',\n '[书]',\n '[亲亲]',\n '[传火炬]',\n '[偷乐]',\n '[偷笑]',\n '[傻眼]',\n '[兔子]',\n '[列表]',\n '[加油啊]',\n '[发红包]',\n '[可怜]',\n '[可爱]',\n '[吃惊]',\n '[呵呵]',\n '[哈哈]',\n '[哈欠]',\n '[哼]',\n '[喵喵]',\n '[嘘]',\n '[嘻嘻]',\n '[噢耶]',\n '[囧]',\n '[困]',\n '[围观]',\n '[太开心]',\n '[失望]',\n '[奥特曼]',\n '[好喜欢]',\n '[好棒]',\n '[好爱哦]',\n '[委屈]',\n '[威武]',\n '[害羞]',\n '[左哼哼]',\n '[巨汗]',\n '[带感]',\n '[干杯]',\n '[幻灯]',\n '[开源]',\n '[弱]',\n '[强]',\n '[微笑]',\n '[心]',\n '[怒]',\n '[怒骂]',\n '[思考]',\n '[悲伤]',\n '[想一想]',\n '[愛你]',\n '[感冒]',\n '[打脸]',\n '[抓狂]',\n '[抱抱]',\n '[拍手]',\n '[拍照]',\n '[拜年了]',\n '[拜拜]',\n '[拳头]',\n '[挖鼻]',\n '[挖鼻屎]',\n '[挤眼]',\n '[捡肥皂]',\n '[推荐]',\n '[握手]',\n '[撒花]',\n '[放鞭炮]',\n '[数据集]',\n '[文章]',\n '[晕]',\n '[最右]',\n '[来]',\n '[求关注]',\n '[汗]',\n '[泪]',\n '[流泪]',\n '[浪]',\n '[浮云]',\n '[熊猫]',\n '[爱你]',\n '[爱红包]',\n '[牛]',\n '[玫瑰]',\n '[生病]',\n '[电影]',\n '[疑问]',\n '[白眼]',\n '[睡觉]',\n '[瞧瞧]',\n '[礼物]',\n '[福到啦]',\n '[竞赛]',\n '[笑哈哈]',\n '[给力]',\n '[耶]',\n '[肥皂]',\n '[膜拜了]',\n '[色]',\n '[花心]',\n '[草泥马]',\n '[蛋糕]',\n '[蜡烛]',\n '[衰]',\n '[要礼物]',\n '[視頻]',\n '[视频]',\n '[许愿]',\n '[论文]',\n '[话筒]',\n '[课程]',\n '[赞]',\n '[赞啊]',\n '[转发]',\n '[酷]',\n '[钟]',\n '[钱]',\n '[问答]',\n '[阳光]',\n '[阴险]',\n '[霹雳]',\n '[音乐]',\n '[顶]',\n '[馋嘴]',\n '[鲜花]',\n '[黑线]',\n '[鼓掌]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_value\n",
    "list_book = []\n",
    "list_emoji=[]\n",
    "for j in array_value:\n",
    "    if len(j[0])!=0:\n",
    "         list_emoji.append(j[0]) \n",
    "    if len(j[1])!=0:\n",
    "        list_book.append(j[1])\n",
    "set_emoji = set()\n",
    "for i in list_emoji:\n",
    "    for j in i:\n",
    "        set_emoji.add(j)\n",
    "set_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_clean[['url','#','brack']] .to_csv('D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp\\\\urlbrackFeature.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['229,方言\\u3000\\u3000安徽方言地图\\u3000\\u3000“安徽方言”不是单一系统的方言，而是多种方言系统的综合体。它既有官话方言，又有非官话方言。\\n',\n '562, 那么多人都迁移到了x86\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(('D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp\\\\temp.txt'),'r')\n",
    "weibos = f.readlines()\n",
    "f.close()\n",
    "weibos[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f58174d998f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# 开始分词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontent_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp\\\\clean_content.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_csv'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 开始分词\n",
    "content_clean['content'].values.to_csv('D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp\\\\clean_content.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =\"D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp/clean_content.txt\"\n",
    "with open(filename,'wb+') as f:\n",
    "    f.writelines([(s+\"\\r\\n\").encode('utf8') for s in  content_clean['content'].values])\n",
    "    # for s in content_clean['content'].values:\n",
    "        # f.write(s.encode('utf8'))\n",
    "        # f.write(\"\\n\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =\"D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp/clean_content_without_other_flag.txt\"\n",
    "\n",
    "with open(filename,'wb+') as f:\n",
    "    for s in  content_clean['content'].values:\n",
    "        te = re.findall(u'[\\u4e00-\\u9fff]+',  s)\n",
    "        f.write((\" \".join(te)+\"\\r\\n\").encode('utf8') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content_clean['content'].values[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_flag(text):\n",
    "    # text =\"\"\n",
    "    english_punctuations = [',','.',':',';','?','!','(',')','[',']','@','&','#','%','$','{','}','--','-']\n",
    "    chinese_punctuations = ['，', '。',', ','：', '；', '？', '（', '）', '【', '】', '！', '@',\n",
    "                        '#', '%', '￥', '*','“','”','《','》','、']\n",
    "    list = []\n",
    "    import re\n",
    "    tag = chinese_punctuations + english_punctuations\n",
    "    for line in text.splitlines():\n",
    "        # l = [word.lower() for word in line.decode().split(\" \") if word not  in tag and (i not in word for i in tag)]\n",
    "        te = re.findall(u'[\\u4e00-\\u9fff]+',  line.decode())\n",
    "        # print(te)\n",
    "        # list .append(\"\".join(te))\n",
    "        list.append(te)\n",
    "    # print(\"\\r\\n\".join(list))\n",
    "    clean_content_delete_flag = \"D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp/clean_content_delete_flag1.txt\"\n",
    "    with open(clean_content_delete_flag,'wb+') as f:\n",
    "        for s  in list:\n",
    "            f.write((\" \".join(s)+\"\\r\\n\").encode('utf8') )\n",
    "clean_content_out=\"D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp/clean_content_out.txt\"\n",
    "with open(clean_content_out,'rb') as f:\n",
    "    text = f.read()\n",
    "    delete_flag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n爱\n文本\n数据分析\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我 爱 文本 数据分析'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "import jieba\n",
    "for i in jieba.cut(\"我爱文本数据分析\",HMM=True):\n",
    "    print(i)\n",
    "lcut = jieba.lcut(\"我爱文本数据分析\",HMM=True)\n",
    "\" \".join(lcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = \"D:\\\\www\\\\data\\\\Weibo Data\\\\Weibo Data\\\\nlp/\"\n",
    "def jieba_token():\n",
    "    filename=dic+\"clean_content_without_other_flag.txt\"\n",
    "    with open(filename,'rb') as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "        with open(dic+\"jieba_result.txt\",'wb+') as g:\n",
    "            for line in text:\n",
    "                lcut = jieba.lcut(line,HMM=True) \n",
    "                g.write((\" \".join(lcut)+\"\\r\\n\").encode('utf8') )\n",
    "jieba_token()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}