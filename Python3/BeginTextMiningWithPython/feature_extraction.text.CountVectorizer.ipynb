{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True,random_state=1,remove=('headers','footers','quotes'))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['description', 'target', 'target_names', 'data', 'filenames', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\",\n \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\",\n \"Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to continue this think tank charade of yours, your\\nfixation on Israel must stop.  You might have to start asking the\\nsame sort of questions of Arab countries as well.  You realize it\\nwould not work, as the Arab countries' treatment of Jews over the\\nlast several decades is so bad that your fixation on Israel would\\nbegin to look like the biased attack that it is.\\n\\n   Everyone in this group recognizes that your stupid 'Center for\\nPolicy Research' is nothing more than a fancy name for some bigot\\nwho hates Israel.\",\n 'Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT\\'s last product in this area (a) was priced over\\n$1000, as I suspect \\'clipper\\' phones will be; (b) came to the customer \\nwith the key automatically preregistered with government authorities. Thus,\\naside from attempting to further legitimize and solidify the fed\\'s posture,\\nClipper seems to be \"more of the same\", rather than a new direction.\\n   Yes, technology will eventually drive the cost down and thereby promote\\nmore widespread use- but at present, the man on the street is not going\\nto purchase a $1000 crypto telephone, especially when the guy on the other\\nend probably doesn\\'t have one anyway.  Am I missing something?\\n   The real question is what the gov will do in a year or two when air-\\ntight voice privacy on a phone line is as close as your nearest pc.  That\\nhas got to a problematic scenario for them, even if the extent of usage\\nnever surpasses the \\'underground\\' stature of PGP.',\n \"Well, I will have to change the scoring on my playoff pool.  Unfortunately\\nI don't have time right now, but I will certainly post the new scoring\\nrules by tomorrow.  Does it matter?  No, you'll enter anyway!!!  Good!\\n\\n--\\n    Keith Keller\\t\\t\\t\\tLET'S GO RANGERS!!!!!\\n\\t\\t\\t\\t\\t\\tLET'S GO QUAKERS!!!!!\\n\\tkkeller@mail.sas.upenn.edu\\t\\tIVY LEAGUE CHAMPS!!!!\",\n \" \\n \\nI read somewhere, I think in Morton Smith's _Jesus the Magician_, that\\nold Lazarus wasn't dead, but going in the tomb was part of an initiation\\nrite for a magi-cult, of which Jesus was also a part.   It appears that\\na 3-day stay was normal.   I wonder .... ?\",\n '\\nOk.  I have a record that shows a IIsi with and without a 64KB cache.\\nIt\\'s small enough that I will attach it.\\n\\nI have also measured some real programs with and without the 64 KB\\ncache.  The speedup varies a lot from app to app, ranging from 0% to\\n40%.  I think an average of 20%-25% is about right.  The subjective\\ndifference is not great, but is sometimes noticable.  A simple cache\\ncard certainly does not transform a IIsi into something enormously\\nbetter.  I do not have an FPU.\\n\\nThe conventional wisdom says that cache cards from all of the makers\\noffer about the same speedup and that there is not much difference\\nbetween 32K and 64K caches.  I bought mine from Third Wave for well\\nunder $150.  I have had absolutely no problems at all with it.\\n\\nIf you get *complete* speedometer runs for a 32K cache, I\\'d like to\\nsee them.  Let\\'s check the conventional wisdom!  The so called\\n\"Performance Rating\" numbers by themselves are of no interest. \\n\\nCheers.\\n\\n(This file must be converted with BinHex 4.0)\\n:#@0KBfKP,Q0`G!\"338083e\"$9!!!!!!\\'A!!!!!$qK3%\"a+!!!!BGJ&CfGiGfH(H\\n)GhQ!QSQBUC!!@SQUU(QSCfPhGhL(H+HCL&KjQTU)LDH)HBL*UCUCJ!U@GQ9hGiK\\nhCAKR9SPiJ)QRQ)QUJ+N(J!UCLD#U#S!!S!QUUTQC#U#DL3J)#3LT#UU)QUUBUT!\\n!S!L3!!UU#!QJS+UT!!QJS*UD#TUUQCQ3!*!!UCFJ!!%c4ACSL\\'D)L)D!#!!)#!!\\n!!!!!!!!)!!!!!!!!J!B8*%9@9L0A\"i!!G`!!G`B!!(J)\"i###B!P[US),B\")21Z\\n-1I\"k-cQFM-VXMHhA!irdjPcVr,lUCVSZ2SI8j@,-l,jPI`F#lZq0A\"AL8XRHjf,\\n6[LJ09\"aZ2TV6l!$9lN@eAP@Rei8(VIpIQkfDK$-ZV[b+9[T5lkC0XZ6LGhf(Ik&\\na$Lkh*Q6-qhh2MIlc*Q2Iq$p([GeSp(ejN!\"bHMdHll$&Qh\\'lR`E26C2(QBqSrMM\\npa-k()jPGXqcpR2rYR9eYd0,*Mh0,h1rj1*hA%pcLHRSG6PF2eIYmc4rIS60EFp+\\nCGE@Vr$[TRAFA(QkA`pG8JkS[@fe1mcBikFQC(,(9K[U&h\"\"0rr\"BDDT(i%XP3Z$\\nV04L8D82FeU01V4K-9U#JaD@1*fZa`EZr3-eGTYkNXH49SjF2Ei[G*5el3[VZ\\'j[\\nVf($bTBHjlEX3Pe0KJ8,ZKH!9Cc3+fJ%kHGZC*BHhNV9+DC6Xd$[S58DFD\"pJ%ei\\nq#CXHkEL`@d%&PYYY\"1f0rG`jm0rJTCYMi4B1KbB\\'pUBQ)PU9\\'q\"*m1miHG#YR`b\\neUNG1\\'mSAP#mR`i-1*K`l[DiNq\\'MQjZA(,4bq\"$*Mimq(KC9@@(-Mc\\'\"f88e9U&0\\nF\\'Y4U5eXb(\"+6T8D@6(R3ae+10Padk\"CAK!*Ea6SThLiA9HF!H&&Da@[,[2bA2!p\\n2VIr&TI)!6V`%S!*eJ#GS!Q!!QqD#2P!*M49m9IdHhm2frUq2Ek))G3e\"Vi)+rQJ\\nC[`%m#+E&0jf\"YI2ql`VI&0qHH!R[339`\\'9hY46)TR+ZkXI!pQRQKCU3%ed9R&Cr\\n!QCiUk+ZmEf)IYI&bqMEffkT5bB`JhYl2K[0PXVe0B@@2*@Uam121D`A`h+cC)Xl\\nIEjf8S+#9`a6[P8p0ZC&6H0ajcY1BR\"JDM3`F%lJ1&5bI+SC2Jh([qeTfVK961rR\\nZVIq[+Rb-TH3\\'B3f0r$h\\'\\'cP%\"UY1\\'jU53jY@5P(RCdPAXAfrl\"Xrhf#Y\"dmV1i$\\n9%Dm@T+f4NMlP5jd-XN0(K5C91\\'R@)4Qb9C5Ke1h%V-kiaRA-NTa`b9(YYL5TM5*\\nF2#bUFFLGJ%,D8QA*9R`eUQ29Sj!!p0b\\'\"c5LEFR4@%9KpDGj1,bijhNaDH,6mrm\\n(3qpJITeraM0+0RHJ*aJ%f`#HJ!R4JJXDK22e!Cab5DK)jkRq0r[IcrC`[c!Krd(\\n$m1VrbJCX!NR)3FrcHYPk(r1CHJjiJ#Hk%\\'J84pq+#+$a2&r&bZ,Ff1V,-KG6qG9\\nMbmUPG9XkUeX$2Gl!Gl!Gl!GE!k5hrX(F4IX4IRNYkb\"M%rSbN4`8m8qPq2rAd[j\\nFhRC#4(PeI2RFhY0+j-GH\\'!P*S)h!#HN!R6JJXb5f\\'b!clJkfb121qGm2MclEe,S\\nmHpf12b4arQ$Q%%PLK\"q(8@I8[qRmmS5[l`\"2fP!\"4CpjY0,DDAp2AlE#eIPBD0c\\nrL1,PeXj39[%9k`HF4Z,ZKGN4h9A+b-T23l)RDf\\'a13X\"\\'-#VbKJ[!9ME*!Tlp2-\\nQckRpM@J2e5BN*f&jHN*[Vp-#f+F(J)PQXJNlYRLpQ3C,%`Cm0l3E[MP\"cXZ6`)B\\nmpVS0)P3Y@XTB5F5qaSr\"XrmrZf1iLXSV,pPVjICFMRrekXdDI`0FHmT[Q!4VL`T\\naalM336chGUr@\"Me6YarIDI&Y2LpE9HPaI#fhNFmq$qLchVC(dUajJ%eb%(6NdIH\\np#jqEd#X1cGDTVmDY965+@Pi,Mr1JeR&pq`q@\"AacVkC[0lZi3-Z-5PZk8%f$Vrd\\nHfR&1mci,3&Nqh9r\"e%\"j5Ve$0rN`AbfB\"Qqlk$C`3@LKQRh0(-MKhNYA+UC&Qhq\\n5kajHR1eFqR,2H5b8Z!SLfG3!!2TPmiF!!3!+58PcD5eMB@0SC3%!!!!)6@0S9(0\\n3C$1R$)JJT`b+33%!ADmicJ!#!!!4a3!!!!!!!!B9!!!!!,AP!!!:\\n-- ',\n \"\\n\\n\\nSounds like wishful guessing.\\n\\n\\n\\n\\n'So-called' ? What do you mean ? How would you see the peace process?\\n\\nSo you say palestineans do not negociate because of 'well-founded' predictions ?\\nHow do you know that they are 'well founded' if you do not test them at the \\ntable ? 18 months did not prove anything, but it's always the other side at \\nfault, right ?\\n\\nWhy ? I do not know why, but if, let's say, the Palestineans (some of them) want\\nALL ISRAEL, and these are known not to be accepted terms by israelis.\\n\\nOr, maybe they (palestinenans) are not yet ready for statehood ?\\n\\nOr, maybe there is too much politics within the palestinean leadership, too many\\nfractions aso ?\\n\\nI am not saying that one of these reasons is indeed the real one, but any of\\nthese could make arabs stall the negotiations.\\n\\n \\nI like California oranges. And the feelings may get sharper at the table.\\n\\n\\n\\nRegards,\",\n \" Nobody is saying that you shouldn't be allowed to use msg.  Just\\ndon't force it on others. If you have food that you want to \\nenhance with msg just put the MSG on the table like salt.  It is\\nthen the option of the eater to use it.  If you make a commerical\\nproduct, just leave it out. You can include a packet (like some\\nsalt packets) if you desire.\\n\\nSalt, pepper, mustard, ketchup, pickles ..... are table options.\\nTreat MSG the same way.  I wouldn't shove my condiments down your\\nthroat, don't shove yours down mine.\\n\\nWFL\\n\",\n \"\\n  I was wondering if anyone can shed any light on just how it is that these\\nelectronic odometers remember the total elapsed mileage?  What kind of\\nmemory is stable/reliable enough, non-volatile enough and independent enough\\n(of outside battery power) to last say, 10 years or more, in the life of a\\nvehicle?  I'm amazed that anything like this could be expected to work for\\nthis length of time (especially in light of all the gizmos I work with that\\nare doing good to work for 2 months without breaking down somehow).\\n\\nSide question:  how about the legal ramifications of selling a used car with\\na replaced odometer that starts over at 0 miles, after say 100/200/300K\\nactual miles.  Looks like fraud would be fairly easy - for the price of a\\nnew odometer, you can say it has however many miles you want to tell the\\nbuyer it has.\\n\\nThanks for any insight.\\n\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples = dataset.data[0:10]\n",
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x845 sparse matrix of type '<class 'numpy.int64'>'\n\twith 1133 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dtm_vectorizer = CountVectorizer()\n",
    "dtm = dtm_vectorizer.fit_transform(data_samples)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(dtm)\n",
    "# dir(dtm)\n",
    "dtm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8450"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = dtm_vectorizer.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x845 sparse matrix of type '<class 'numpy.int64'>'\n\twith 1133 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_vectorizer.inverse_transform(dtm)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x7 sparse matrix of type '<class 'numpy.int64'>'\n\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['This is a text mining book.',\n",
    "          'Is this a text mining book?',\n",
    "          'Text mining with python.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer .fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book', 'is', 'mining', 'python', 'text', 'this', 'with']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 0],\n       [1, 1, 1, 0, 1, 1, 0],\n       [0, 0, 1, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n        tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n       [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "print(bigram_vectorizer)\n",
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0],\n       [1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer2 = CountVectorizer(ngram_range=(1,3))\n",
    "x2= bigram_vectorizer2.fit_transform(corpus).toarray()\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()\n",
    "bigram_vectorizer.get_stop_words()\n",
    "bigram_vectorizer.get_params()\n",
    "bigram_vectorizer.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x50 sparse matrix of type '<class 'numpy.float64'>'\n\twith 393 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hashing_vectorizer = HashingVectorizer(n_features=50)\n",
    "hashing_dtm = hashing_vectorizer.fit_transform(data_samples)\n",
    "hashing_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.0805823 , -0.04029115,  0.04029115, -0.04029115,\n        -0.12087344,  0.        ,  0.04029115, -0.68494952,  0.12087344,\n         0.        , -0.12087344, -0.04029115, -0.04029115, -0.04029115,\n        -0.0805823 ,  0.        , -0.04029115,  0.12087344, -0.16116459,\n        -0.04029115,  0.        ,  0.        ,  0.12087344, -0.04029115,\n         0.04029115,  0.12087344,  0.40291148,  0.04029115,  0.12087344,\n         0.        ,  0.        , -0.04029115,  0.04029115, -0.04029115,\n        -0.04029115,  0.04029115,  0.        ,  0.12087344,  0.04029115,\n         0.0805823 ,  0.24174689, -0.04029115, -0.04029115,  0.04029115,\n        -0.28203804,  0.16116459, -0.04029115,  0.        ,  0.0805823 ],\n       [-0.07905694,  0.07905694, -0.07905694,  0.07905694,  0.        ,\n        -0.15811388,  0.07905694,  0.07905694, -0.07905694,  0.07905694,\n         0.07905694,  0.        ,  0.07905694, -0.23717082, -0.07905694,\n        -0.07905694,  0.        ,  0.        ,  0.        ,  0.07905694,\n         0.        ,  0.        , -0.07905694,  0.        ,  0.        ,\n         0.07905694, -0.07905694,  0.07905694,  0.        ,  0.        ,\n        -0.07905694,  0.        , -0.07905694,  0.15811388,  0.        ,\n         0.        ,  0.        , -0.23717082,  0.55339859, -0.07905694,\n         0.        ,  0.15811388,  0.        , -0.07905694,  0.        ,\n        -0.23717082,  0.55339859,  0.        , -0.07905694,  0.07905694],\n       [-0.05933908,  0.05933908,  0.11867817,  0.05933908, -0.05933908,\n        -0.11867817,  0.05933908,  0.05933908, -0.3560345 ,  0.        ,\n         0.05933908,  0.        ,  0.05933908, -0.23735633,  0.        ,\n        -0.05933908, -0.05933908,  0.        ,  0.11867817,  0.17801725,\n        -0.05933908, -0.11867817,  0.05933908,  0.        , -0.05933908,\n        -0.29669541,  0.05933908,  0.23735633,  0.05933908, -0.05933908,\n         0.        ,  0.        ,  0.23735633,  0.05933908, -0.23735633,\n         0.        , -0.05933908,  0.        ,  0.29669541,  0.        ,\n         0.05933908,  0.29669541,  0.05933908,  0.05933908, -0.05933908,\n        -0.17801725,  0.41537358,  0.        ,  0.        , -0.11867817],\n       [ 0.        ,  0.05598925, -0.05598925,  0.        , -0.05598925,\n        -0.1119785 ,  0.        ,  0.        , -0.61588176,  0.        ,\n        -0.05598925,  0.        ,  0.223957  ,  0.        , -0.16796775,\n         0.        , -0.05598925,  0.        ,  0.1119785 , -0.223957  ,\n         0.        , -0.05598925,  0.        ,  0.        , -0.05598925,\n        -0.05598925,  0.27994626,  0.223957  , -0.05598925,  0.05598925,\n        -0.05598925,  0.        ,  0.05598925, -0.1119785 ,  0.        ,\n        -0.05598925, -0.1119785 ,  0.05598925,  0.223957  ,  0.05598925,\n         0.16796775,  0.223957  , -0.16796775,  0.        ,  0.16796775,\n        -0.05598925,  0.223957  , -0.05598925, -0.1119785 ,  0.1119785 ],\n       [ 0.        ,  0.        , -0.28867513,  0.14433757,  0.        ,\n         0.        ,  0.        ,  0.        , -0.28867513,  0.        ,\n         0.        ,  0.14433757,  0.        , -0.14433757,  0.        ,\n         0.14433757,  0.        ,  0.57735027, -0.14433757,  0.14433757,\n         0.        ,  0.        ,  0.        , -0.14433757,  0.        ,\n         0.14433757,  0.        ,  0.        ,  0.14433757,  0.        ,\n         0.14433757,  0.        ,  0.14433757,  0.        ,  0.        ,\n         0.14433757,  0.14433757,  0.14433757,  0.14433757,  0.        ,\n         0.14433757, -0.14433757,  0.        ,  0.        , -0.28867513,\n         0.14433757,  0.14433757,  0.        ,  0.        ,  0.14433757],\n       [ 0.13867505, -0.13867505,  0.        , -0.13867505,  0.        ,\n         0.        ,  0.        ,  0.13867505, -0.2773501 , -0.2773501 ,\n         0.        , -0.2773501 ,  0.        , -0.2773501 ,  0.13867505,\n         0.        ,  0.        ,  0.        ,  0.13867505,  0.        ,\n         0.        ,  0.41602515,  0.        ,  0.        ,  0.        ,\n        -0.13867505,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.13867505,  0.        ,\n         0.        ,  0.13867505,  0.        ,  0.        ,  0.13867505,\n        -0.13867505,  0.        ,  0.        ,  0.13867505,  0.13867505,\n        -0.2773501 ,  0.41602515,  0.        ,  0.13867505,  0.        ],\n       [ 0.10998534,  0.10998534, -0.14664712,  0.07332356,  0.        ,\n        -0.10998534,  0.        ,  0.03666178, -0.36661779, -0.14664712,\n        -0.10998534,  0.03666178,  0.03666178, -0.14664712,  0.03666178,\n         0.14664712,  0.14664712, -0.14664712,  0.18330889,  0.03666178,\n         0.        , -0.07332356, -0.07332356,  0.10998534, -0.03666178,\n        -0.07332356, -0.18330889,  0.25663245,  0.07332356,  0.        ,\n        -0.18330889, -0.07332356, -0.18330889,  0.07332356, -0.21997067,\n        -0.21997067,  0.        ,  0.07332356,  0.29329423,  0.        ,\n        -0.21997067, -0.03666178, -0.10998534,  0.10998534, -0.25663245,\n        -0.25663245,  0.14664712,  0.03666178, -0.03666178, -0.10998534],\n       [-0.04598005,  0.04598005, -0.04598005,  0.04598005,  0.04598005,\n        -0.1839202 ,  0.        ,  0.        , -0.41382044,  0.        ,\n         0.04598005,  0.27588029,  0.13794015, -0.13794015,  0.0919601 ,\n         0.        ,  0.        ,  0.        ,  0.        , -0.13794015,\n         0.        ,  0.        ,  0.        ,  0.        , -0.0919601 ,\n         0.        , -0.27588029,  0.0919601 ,  0.04598005,  0.0919601 ,\n        -0.0919601 , -0.04598005,  0.        ,  0.13794015,  0.0919601 ,\n         0.13794015,  0.        ,  0.        ,  0.59774064,  0.04598005,\n         0.        ,  0.        , -0.04598005, -0.04598005,  0.0919601 ,\n        -0.22990024,  0.13794015, -0.1839202 ,  0.        , -0.04598005],\n       [ 0.        ,  0.28571429,  0.07142857,  0.        ,  0.07142857,\n         0.07142857,  0.07142857,  0.        , -0.42857143,  0.07142857,\n         0.28571429, -0.07142857,  0.14285714,  0.        ,  0.        ,\n         0.        , -0.07142857,  0.        ,  0.21428571,  0.        ,\n         0.14285714,  0.        , -0.07142857,  0.        , -0.07142857,\n         0.        , -0.07142857,  0.14285714,  0.21428571,  0.        ,\n         0.        ,  0.        ,  0.14285714,  0.        , -0.14285714,\n         0.        , -0.07142857, -0.14285714,  0.42857143, -0.07142857,\n         0.        ,  0.21428571,  0.        , -0.07142857, -0.07142857,\n         0.07142857,  0.35714286, -0.07142857,  0.        ,  0.        ],\n       [ 0.        ,  0.18786729,  0.        ,  0.06262243, -0.06262243,\n        -0.18786729, -0.12524486,  0.        , -0.25048972,  0.06262243,\n         0.        , -0.06262243,  0.        , -0.18786729,  0.12524486,\n        -0.06262243,  0.06262243, -0.12524486,  0.438357  ,  0.        ,\n        -0.06262243,  0.        , -0.06262243,  0.06262243,  0.12524486,\n        -0.06262243, -0.06262243,  0.12524486,  0.06262243,  0.06262243,\n         0.        ,  0.06262243,  0.31311215,  0.        , -0.06262243,\n         0.        , -0.12524486, -0.12524486,  0.06262243,  0.        ,\n        -0.12524486,  0.31311215,  0.06262243,  0.25048972,  0.06262243,\n        -0.25048972,  0.31311215, -0.06262243, -0.06262243, -0.12524486]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   0.,  33.],\n       [  0.,   1.,   0.,  12.],\n       [  0.,   0.,   1.,  18.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements = [{'city': 'Dubai', 'temperature': 33.},\n",
    "                {'city': 'London', 'temperature': 12.},\n",
    "                {'city': 'San Fransisco', 'temperature': 18.}]\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False)#可选变量，是否生成 scipy.sparse 矩阵\n",
    "measurements_vec = vec.fit_transform(measurements)\n",
    "measurements_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'city=Dubai': 1.0, 'temperature': 33.0},\n {'city=London': 1.0, 'temperature': 12.0},\n {'city=San Fransisco': 1.0, 'temperature': 18.0}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.vocabulary_\n",
    "vec.inverse_transform(measurements_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  1.],\n       [ 3.,  0.,  1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = [{'text': 1, 'mining': 2}, {'Python': 3, 'text': 1}]\n",
    "X3 = vec.fit_transform(D)\n",
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Python': 0, 'mining': 1, 'text': 2}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mining': 2.0, 'text': 1.0}, {'Python': 3.0, 'text': 1.0}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.inverse_transform(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4857976bd4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m corpus = ['This is a text mining book',\n\u001b[1;32m      4\u001b[0m           \u001b[1;34m'Is this a text mining book'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           'Text mining with python']\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gensim'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "corpus = ['This is a text mining book',\n",
    "          'Is this a text mining book',\n",
    "          'Text mining with python']\n",
    "\n",
    "texts = [[word for word in document.lower().split()] for document in corpus]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.keys())\n",
    "print(dictionary.values())\n",
    "print(dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ec15f3ffe0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gensim'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x3 sparse matrix of type '<class 'numpy.float64'>'\n\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = [[4, 0, 0],\n",
    "      [3, 2, 0],\n",
    "      [3, 0, 0],\n",
    "      [3, 0, 2]]\n",
    "tfidf = transformer.fit_transform(tf)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ],\n       [ 0.61638324,  0.78744632,  0.        ],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.61638324,  0.        ,  0.78744632]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizerTfidf= TfidfVectorizer()\n",
    "data_samples_tfidf = vectorizerTfidf.fit_transform(data_samples)\n",
    "data_samples_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0fhmt',\n '0jf',\n '0kbfkp',\n '0lzi3',\n '0pxve0b',\n '0qhh',\n '0rhj',\n '0rn',\n '0rr',\n '0s9',\n '0sc3',\n '10',\n '100',\n '1000',\n '10padk',\n '150',\n '18',\n '1f0rg',\n '1i',\n '1mci',\n '1r',\n '20',\n '200',\n '21z',\n '25',\n '2ba2',\n '2fp',\n '2gl',\n '2h5b8z',\n '2p',\n '2tpmif',\n '2vir',\n '300k',\n '32k',\n '33',\n '338083e',\n '339',\n '3c',\n '3frchypk',\n '3lt',\n '3qpjiteram0',\n '40',\n '4a3',\n '4bq',\n '4cpjy0',\n '4qb9c5ke1h',\n '4vl',\n '58pcd5emb',\n '5bi',\n '5el3',\n '5kajhr1efqr',\n '5p',\n '5pzk8',\n '64',\n '64k',\n '64kb',\n '6h0ajcy1br',\n '6mrm',\n '6ndih',\n '6t8d',\n '6v',\n '8m8qpq2rad',\n '9cc3',\n '9hy46',\n '9k',\n '9kpdgj1',\n '9l0a',\n '9ln',\n '9me',\n '9r',\n '9u',\n '_jesus',\n 'a13x',\n 'a2',\n 'a6',\n 'aacvkc',\n 'aalm336chgur',\n 'abfb',\n 'about',\n 'absolutely',\n 'accept',\n 'accepted',\n 'acts',\n 'actual',\n 'actually',\n 'admicj',\n 'after',\n 'air',\n 'aj',\n 'al8xrhjf',\n 'all',\n 'allowed',\n 'also',\n 'alt',\n 'although',\n 'always',\n 'am',\n 'amazed',\n 'an',\n 'and',\n 'any',\n 'anyone',\n 'anything',\n 'anyway',\n 'ap',\n 'app',\n 'appears',\n 'arab',\n 'arabs',\n 'are',\n 'area',\n 'as',\n 'aside',\n 'ask',\n 'asking',\n 'aso',\n 'at',\n 'atheism',\n 'atheist',\n 'atrocities',\n 'att',\n 'attach',\n 'attack',\n 'attempting',\n 'austria',\n 'authorities',\n 'automatically',\n 'average',\n 'away',\n 'az2tv6l',\n 'b3f0r',\n 'b8',\n 'b9',\n 'bad',\n 'bake',\n 'battery',\n 'bddt',\n 'be',\n 'because',\n 'begin',\n 'better',\n 'between',\n 'bgj',\n 'bhhnv9',\n 'bhmdhll',\n 'biased',\n 'big',\n 'bigot',\n 'bijhnadh',\n 'binhex',\n 'blessing',\n 'bought',\n 'bqmeffkt5bb',\n 'breaking',\n 'btbhjlex3pe0kj8',\n 'bufflgj',\n 'bummin',\n 'but',\n 'buyer',\n 'by',\n 'bye',\n 'bz',\n 'c4acsl',\n 'c5lefr4',\n 'cab5dk',\n 'cache',\n 'caches',\n 'cak',\n 'california',\n 'called',\n 'came',\n 'can',\n 'car',\n 'card',\n 'cards',\n 'cc',\n 'center',\n 'certainly',\n 'cfgigfh',\n 'cge',\n 'champs',\n 'change',\n 'charade',\n 'check',\n 'cheers',\n 'chewables',\n 'clearly',\n 'clipper',\n 'cljkfb121qgm2mclee',\n 'close',\n 'cm0l3e',\n 'commerical',\n 'commited',\n 'complete',\n 'condiments',\n 'continue',\n 'conventional',\n 'converted',\n 'cost',\n 'could',\n 'countries',\n 'cp',\n 'cqfm',\n 'cr',\n 'crypto',\n 'cult',\n 'customer',\n 'cxhkel',\n 'cxz6',\n 'd8qa',\n 'da',\n 'daily',\n 'day',\n 'dc6xd',\n 'ddap2ale',\n 'dead',\n 'decades',\n 'degree',\n 'denial',\n 'described',\n 'desire',\n 'did',\n 'difference',\n 'dinq',\n 'direction',\n 'disagree',\n 'dl3j',\n 'dm',\n 'dmv1i',\n 'do',\n 'does',\n 'doesn',\n 'doing',\n 'don',\n 'down',\n 'drive',\n 'duajj',\n 'e26c2',\n 'ea6sthlia9hf',\n 'eap',\n 'easy',\n 'eater',\n 'eb',\n 'ed9r',\n 'edu',\n 'egtyknxh49sjf2ei',\n 'ei',\n 'eipbd0c',\n 'ej',\n 'ejn',\n 'elapsed',\n 'electronic',\n 'end',\n 'enhance',\n 'enormously',\n 'enough',\n 'enter',\n 'especially',\n 'etc',\n 'eung1',\n 'euq29sj',\n 'europe',\n 'europeans',\n 'even',\n 'eventually',\n 'ever',\n 'everyone',\n 'existance',\n 'expect',\n 'expected',\n 'extent',\n 'ezr3',\n 'f2',\n 'f4ix4irnykb',\n 'f4nmlp5jd',\n 'f88e9u',\n 'fairly',\n 'faith',\n 'fancy',\n 'faq',\n 'fault',\n 'fe1mcbikfqc',\n 'fed',\n 'feelings',\n 'ff1v',\n 'fhnfmq',\n 'fhrc',\n 'file',\n 'fixation',\n 'fj',\n 'flintstone',\n 'food',\n 'for',\n 'force',\n 'forget',\n 'founded',\n 'fpu',\n 'fractions',\n 'fraud',\n 'from',\n 'further',\n 'fuss',\n 'fza',\n 'g3e',\n 'ge',\n 'gesp',\n 'get',\n 'gh',\n 'ghq',\n 'gizmos',\n 'gl',\n 'go',\n 'going',\n 'good',\n 'got',\n 'gov',\n 'government',\n 'gq9hgik',\n 'great',\n 'group',\n 'gs',\n 'guessing',\n 'guilt',\n 'guy',\n 'h1rj1',\n 'ha',\n 'had',\n 'happily',\n 'hard',\n 'has',\n 'hates',\n 'have',\n 'having',\n 'hbl',\n 'hcakr9spij',\n 'hcl',\n 'hf4z',\n 'hfr',\n 'hj',\n 'hk',\n 'hn',\n 'holocaust',\n 'how',\n 'however',\n 'i8',\n 'icrc',\n 'iejf8s',\n 'if',\n 'ignore',\n 'iii',\n 'iisi',\n 'ik',\n 'in',\n 'incidences',\n 'include',\n 'indeed',\n 'independent',\n 'inhuman',\n 'initiation',\n 'insight',\n 'interest',\n 'into',\n 'irdjpcvr',\n 'is',\n 'israel',\n 'israeli',\n 'israelis',\n 'israels',\n 'it',\n 'ivy',\n 'iyi',\n 'j2e5bn',\n 'j5ve',\n 'j84pq',\n 'jad',\n 'jdm3',\n 'jesus',\n 'jews',\n 'jhn',\n 'jhyl2k',\n 'jim',\n 'jimmy',\n 'jjt',\n 'jkrq0r',\n 'jm0rjtcymi4b1kbb',\n 'jpgxqcpr2ryr9eyd0',\n 'jpi',\n 'jqed',\n 'ju53jy',\n 'just',\n 'k5c91',\n 'k5hrx',\n 'kb',\n 'kc9',\n 'keith',\n 'keller',\n 'ketchup',\n 'key',\n 'kg6qg9',\n 'khgzc',\n 'kiara',\n 'kind',\n 'kjqtu',\n 'kkeller',\n 'know',\n 'known',\n 'krd',\n 'l3',\n 'last',\n 'lazarus',\n 'ldh',\n 'leadership',\n 'league',\n 'leap',\n 'least',\n 'leave',\n 'legal',\n 'legitimate',\n 'legitimize',\n 'length',\n 'let',\n 'letter',\n 'life',\n 'light',\n 'like',\n 'line',\n 'little',\n 'lived',\n 'lj09',\n 'lj1',\n 'lkh',\n 'lkqrh0',\n 'll',\n 'logic',\n 'look',\n 'looks',\n 'lot',\n 'lr',\n 'lucvsz2si8j',\n 'lzq0a',\n 'm1mihg',\n 'm1vrbjcx',\n 'm49m9idhhm2fruq2ek',\n 'magi',\n 'magician_',\n 'mail',\n 'make',\n 'makers',\n 'makes',\n 'man',\n 'many',\n 'matter',\n 'may',\n 'maybe',\n 'mbmupg9xkuex',\n 'mc',\n 'me6yaridi',\n 'mean',\n 'measured',\n 'media',\n 'memory',\n 'mh0',\n 'mhpf12b4arq',\n 'might',\n 'mileage',\n 'miles',\n 'mimq',\n 'mine',\n 'missing',\n 'mkhnya',\n 'months',\n 'more',\n 'morton',\n 'most',\n 'mp',\n 'mpvs0',\n 'mqjza',\n 'mr',\n 'mr1jer',\n 'msap',\n 'msg',\n 'much',\n 'must',\n 'mustard',\n 'my',\n 'nad',\n 'name',\n 'nearest',\n 'need',\n 'negociate',\n 'negotiations',\n 'never',\n 'new',\n 'newsgroup',\n 'no',\n 'nobody',\n 'non',\n 'normal',\n 'not',\n 'nothing',\n 'noticable',\n 'notwithstanding',\n 'now',\n 'nqh9r',\n 'nr',\n 'nta',\n 'numbers',\n 'occured',\n 'odometer',\n 'odometers',\n 'of',\n 'offer',\n 'oh',\n 'ok',\n 'old',\n 'on',\n 'one',\n 'option',\n 'options',\n 'or',\n 'oranges',\n 'other',\n 'others',\n 'out',\n 'outside',\n 'over',\n 'p0b',\n 'p3y',\n 'p8p0zc',\n 'pa',\n 'packet',\n 'packets',\n 'palestinean',\n 'palestineans',\n 'palestinenans',\n 'part',\n 'pc',\n 'pclhrsg6pf2eiymc4ris60efp',\n 'peace',\n 'pei2rfhy0',\n 'people',\n 'pepper',\n 'performance',\n 'pexj39',\n 'pg8jks',\n 'pgp',\n 'phone',\n 'phones',\n 'pi',\n 'pickles',\n 'pity',\n 'pj',\n 'playoff',\n 'plk',\n 'points',\n 'policy',\n 'politics',\n 'pool',\n 'post',\n 'posture',\n 'power',\n 'ppvjicfmrrekxddi',\n 'pq',\n 'pqrqkcu3',\n 'pqxjnlyrlpq3c',\n 'predictions',\n 'preregistered',\n 'present',\n 'pretend',\n 'price',\n 'priced',\n 'principle',\n 'privacy',\n 'pro',\n 'probably',\n 'problematic',\n 'problems',\n 'process',\n 'product',\n 'programs',\n 'promote',\n 'proposal',\n 'prove',\n 'pu9',\n 'pubq',\n 'purchase',\n 'put',\n 'pyyy',\n 'q0',\n 'q2iq',\n 'q6',\n 'qbqsrmm',\n 'qciuk',\n 'qckrpm',\n 'qetfvk961rr',\n 'qh',\n 'qhh2milc',\n 'qhq',\n 'qjs',\n 'qk3',\n 'qka',\n 'qlchvc',\n 'qqd',\n 'qqlk',\n 'qrmms5',\n 'qrq',\n 'qscfphghl',\n 'qsqbuc',\n 'quakers',\n 'question',\n 'questions',\n 'quj',\n 'quubut',\n 'quutqc',\n 'r1chjjij',\n 'r3ae',\n 'r4jjxdk22e',\n 'r6jjxb5f',\n 'races',\n 'ramifications',\n 'rangers',\n 'ranging',\n 'rather',\n 'rating',\n 'rb',\n 'rcdpaxafrl',\n 'rdf',\n 'read',\n 'ready',\n 'real',\n 'realize',\n 'reason',\n 'reasons',\n 'received',\n 'recognizes',\n 'record',\n 'rediculous',\n 'regards',\n 'rei8',\n 'reliable',\n 'remember',\n 'replaced',\n 'report',\n 'reports',\n 'reputation',\n 'research',\n 'right',\n 'rite',\n 'rl1',\n 'rqj',\n 'rsbn4',\n 'ruin',\n 'rules',\n 'runs',\n 's58dfd',\n 'salt',\n 'same',\n 'sas',\n 'say',\n 'saying',\n 'says',\n 'sc2jh',\n 'scenario',\n 'scoring',\n 'see',\n 'seem',\n 'seems',\n 'selling',\n 'several',\n 'shame',\n 'sharper',\n 'shed',\n 'shouldn',\n 'shove',\n 'shows',\n 'side',\n 'simple',\n 'slfg3',\n 'small',\n 'smith',\n 'so',\n 'soldiers',\n 'solidify',\n 'some',\n 'somehow',\n 'something',\n 'sometimes',\n 'somewhere',\n 'sorry',\n 'sort',\n 'sounds',\n 'speedometer',\n 'speedup',\n 'squu',\n 'stable',\n 'stall',\n 'start',\n 'starts',\n 'statehood',\n 'statement',\n 'stature',\n 'stay',\n 'steam',\n 'still',\n 'stop',\n 'story',\n 'street',\n 'strongest',\n 'stupid',\n 'subjective',\n 'subsidizing',\n 'such',\n 'sure',\n 'surpasses',\n 'suspect',\n 't23l',\n 't5lkc0xz6lghf',\n 'table',\n 'tank',\n 'technology',\n 'telephone',\n 'tell',\n 'terms',\n 'test',\n 'th3',\n 'than',\n 'thanks',\n 'that',\n 'the',\n 'them',\n 'themselves',\n 'then',\n 'there',\n 'thereby',\n 'these',\n 'they',\n 'think',\n 'third',\n 'this',\n 'throat',\n 'thus',\n 'ti',\n 'tight',\n 'time',\n 'timmons',\n 'tlp2',\n 'to',\n 'tomb',\n 'tomorrow',\n 'too',\n 'total',\n 'tr',\n 'trafa',\n 'transform',\n 'treat',\n 'treating',\n 'treatment',\n 'try',\n 'tuuqcq3',\n 'two',\n 'uam121d',\n 'uc',\n 'ucfj',\n 'ucld',\n 'ucucj',\n 'ud',\n 'under',\n 'underground',\n 'unfortunate',\n 'unfortunately',\n 'upenn',\n 'us',\n 'usage',\n 'use',\n 'used',\n 'ut',\n 'uu',\n 'uy1',\n 'v04l8d82feu01v4k',\n 'varies',\n 'vbkj',\n 'vehicle',\n 'vf',\n 'vi',\n 'vipiqkfdk',\n 'voice',\n 'volatile',\n 'vp',\n 'vr',\n 'vrd',\n 'vxmhha',\n 'vz',\n 'want',\n 'was',\n 'wasn',\n 'wave',\n 'way',\n 'well',\n 'wfl',\n 'what',\n 'when',\n 'which',\n 'who',\n 'whole',\n 'why',\n 'widespread',\n 'will',\n 'wisdom',\n 'wishful',\n 'with',\n 'within',\n 'without',\n 'won',\n 'wonder',\n 'wondering',\n 'work',\n 'world',\n 'would',\n 'wouldn',\n 'x1cgdtvmdy965',\n 'xl',\n 'xn0',\n 'xp3z',\n 'xrhf',\n 'xrmrzf1ilxsv',\n 'xtb5f5qasr',\n 'y2lpe9hpai',\n 'y4u5exb',\n 'yeah',\n 'year',\n 'years',\n 'yes',\n 'yet',\n 'yi2ql',\n 'you',\n 'your',\n 'yours',\n 'yr',\n 'yyl5tm5',\n 'zkgn4h9a',\n 'zkh',\n 'zkxi',\n 'zmef',\n 'zv',\n 'zviq']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizerTfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True,random_state=1,remove=('headers','footers','quotes'),categories=['rec.sport.baseball'])\n",
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ..., \n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dtm_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "dtm = dtm_vectorizer.fit_transform(dataset.data).toarray()\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(dtm).toarray()\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizerTfidf = TfidfVectorizer()\n",
    "tfidf = vectorizerTfidf.fit_transform(dataset.data).toarray()\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-81ffe6c0f088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '!', '@',\n\u001b[1;32m      7\u001b[0m                         '#', '%', '$', '*']\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mwords_lower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '!', '@',\n",
    "                        '#', '%', '$', '*']\n",
    "words = [word_tokenize(t) for t in dataset.data]\n",
    "words_lower = [[j.lower() for j in i] for i in words]\n",
    "\n",
    "print(words_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clear =[]\n",
    "for i in words_lower:\n",
    "    words_filter = []\n",
    "    for  j in i :\n",
    "        if j not in english_stopwords:\n",
    "            if j not in english_punctuations:\n",
    "                words_filter.append(j)\n",
    "                \n",
    "    word_clear.append(words_filter)\n",
    "word_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gensim\n",
    "model = gensim.models.Word2Vec(word_clear,size=100,window=5,min_count=5)\n",
    "set(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['sorry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"\") as f :\n",
    "    read = f.readlines()\n",
    "title=[]\n",
    "for i in read[0:500]:\n",
    "    title.append(i.split(\"|\")[1].decode(\"utf-8\"))\n",
    "import jieba\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    read = f.read().decode(\"utf-8\")\n",
    "stop_words = read.splitlines()\n",
    "texts=[]\n",
    "for i in title:\n",
    "    title_seg=[]\n",
    "    segs = jieba.cut(i)\n",
    "    for seg in segs:\n",
    "        if seg not in stop_words:\n",
    "            title_seg.append(seg)\n",
    "            \n",
    "    texts.append(title_seg)\n",
    "    texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "word_count = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-902dc2300494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gensim'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "sentences = [[j.lower() for j in i ] for i in brown.sents()]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}