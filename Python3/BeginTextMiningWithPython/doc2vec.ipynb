{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load_word2vec_format(\"vectors.txt\",binary=False)\n",
    "model = Word2Vec.load_word2vec_format(\"vectors.bin\",binary=True)\n",
    "model.most_similar(positive=['woman','king'],negative=['man'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('food_words.txt','r') as infile:\n",
    "    food_words = infile.readlines()\n",
    "with open('sports_words.txt','r') as infile:\n",
    "    sports_words = infile.readlines(0)\n",
    "with open('weather_words.txt','r') as infile:\n",
    "    weather_words = infile.readlines()\n",
    "def getWordVecs(words):\n",
    "    vecs =[]\n",
    "    for word in words:\n",
    "        word = word.replace('\\n','')\n",
    "        try :\n",
    "            vecs.append(model[word].reshape((1,300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs,dtype='float')\n",
    "\n",
    "food_vecs = getWordVecs(food_words)\n",
    "sports_vecs = getWordVecs(sports_words)\n",
    "weather_vecs = getWordVecs(weather_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "ts = TSNE(2)\n",
    "reduce_vecs = ts.fit_transform(np.concatenate((food_vecs,sports_vecs,weather_vecs)))\n",
    "\n",
    "for i in range(len(reduce_vecs)):\n",
    "    if i < len(food_vecs):\n",
    "        color = 'b'\n",
    "    elif i >= len(food_vecs) and i < (len(sports_vecs)+len(weather_vecs)):\n",
    "        color = 'r'\n",
    "    else:\n",
    "        color = 'g'\n",
    "    plt.plot(reduce_vecs[i,0],reduce_vecs[i,1],marker ='o',color\n",
    "             =color,marksize=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "with open('twitter_data/pos_tweets.txt', 'r') as infile:\n",
    "    pos_tweets = infile.readlines()\n",
    "\n",
    "with open('twitter_data/neg_tweets.txt', 'r') as infile:\n",
    "    neg_tweets = infile.readlines()\n",
    "\n",
    "#use 1 for positive sentiment, 0 for negative\n",
    "y = np.concatenate((np.ones(len(pos_tweets)), np.zeros(len(neg_tweets))))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos_tweets, neg_tweets)), y, test_size=0.2)\n",
    "\n",
    "#Do some very minor text preprocessing\n",
    "def cleanText(corpus):\n",
    "    corpus = [z.lower().replace('\\n','').split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "x_train = cleanText(x_train)\n",
    "x_test = cleanText(x_test)\n",
    "\n",
    "n_dim = 300\n",
    "#Initialize model and build vocab\n",
    "imdb_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "imdb_w2v.build_vocab(x_train)\n",
    "\n",
    "#Train the model over train_reviews (this may take several minutes)\n",
    "imdb_w2v.train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "with open('IMDB_data/pos.txt','r') as infile:\n",
    "    pos_reviews = infile.readlines()\n",
    "\n",
    "with open('IMDB_data/neg.txt','r') as infile:\n",
    "    neg_reviews = infile.readlines()\n",
    "\n",
    "with open('IMDB_data/unsup.txt','r') as infile:\n",
    "    unsup_reviews = infile.readlines()\n",
    "\n",
    "#use 1 for positive sentiment, 0 for negative\n",
    "y = np.concatenate((np.ones(len(pos_reviews)), np.zeros(len(neg_reviews))))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos_reviews, neg_reviews)), y, test_size=0.2)\n",
    "\n",
    "#Do some very minor text preprocessing\n",
    "def cleanText(corpus):\n",
    "    punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "    corpus = [z.lower().replace('\\n','') for z in corpus]\n",
    "    corpus = [z.replace('<br />', ' ') for z in corpus]\n",
    "\n",
    "    #treat punctuation as individual words\n",
    "    for c in punctuation:\n",
    "        corpus = [z.replace(c, ' %s '%c) for z in corpus]\n",
    "    corpus = [z.split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "x_train = cleanText(x_train)\n",
    "x_test = cleanText(x_test)\n",
    "unsup_reviews = cleanText(unsup_reviews)\n",
    "\n",
    "#Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "#We do this by using the LabeledSentence method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "#a dummy index of the review.\n",
    "def labelizeReviews(reviews, label_type):\n",
    "    labelized = []\n",
    "    for i,v in enumerate(reviews):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeReviews(x_train, 'TRAIN')\n",
    "x_test = labelizeReviews(x_test, 'TEST')\n",
    "unsup_reviews = labelizeReviews(unsup_reviews, 'UNSUP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "size = 400\n",
    "\n",
    "#instantiate our DM and DBOW models\n",
    "model_dm = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, workers=3)\n",
    "model_dbow = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, dm=0, workers=3)\n",
    "\n",
    "#build vocab over all reviews\n",
    "model_dm.build_vocab(np.concatenate((x_train, x_test, unsup_reviews)))\n",
    "model_dbow.build_vocab(np.concatenate((x_train, x_test, unsup_reviews)))\n",
    "\n",
    "#We pass through the data set multiple times, shuffling the training reviews each time to improve accuracy.\n",
    "all_train_reviews = np.concatenate((x_train, unsup_reviews))\n",
    "for epoch in range(10):\n",
    "    # return 随机洗牌后的副本\n",
    "    perm = np.random.permutation(all_train_reviews.shape[0])\n",
    "    model_dm.train(all_train_reviews[perm])\n",
    "    model_dbow.train(all_train_reviews[perm])\n",
    "\n",
    "#Get training set vectors from our models\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model[z.labels[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.concatenate(vecs)\n",
    "\n",
    "train_vecs_dm = getVecs(model_dm, x_train, size)\n",
    "train_vecs_dbow = getVecs(model_dbow, x_train, size)\n",
    "\n",
    "train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))\n",
    "\n",
    "#train over test set\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "for epoch in range(10):\n",
    "    perm = np.random.permutation(x_test.shape[0])\n",
    "    model_dm.train(x_test[perm])\n",
    "    model_dbow.train(x_test[perm])\n",
    "\n",
    "#Construct vectors for test reviews\n",
    "test_vecs_dm = getVecs(model_dm, x_test, size)\n",
    "test_vecs_dbow = getVecs(model_dbow, x_test, size)\n",
    "\n",
    "test_vecs = np.hstack((test_vecs_dm, test_vecs_dbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(train_vecs, y_train)\n",
    "\n",
    "print ('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_probas = lr.predict_proba(test_vecs)[:,1]\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_test, pred_probas)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NNet import NeuralNet\n",
    "# \n",
    "# nnet = NeuralNet(50, learn_rate=1e-2)\n",
    "# maxiter = 500\n",
    "# batch = 150\n",
    "# _ = nnet.fit(train_vecs, y_train, fine_tune=False, maxiter=maxiter, SGD=True, batch=batch, rho=0.9)\n",
    "# \n",
    "# print 'Test Accuracy: %.2f'%nnet.score(test_vecs, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}